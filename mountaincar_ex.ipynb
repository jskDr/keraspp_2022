{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17cc6be-b733-42d7-96e2-137159243e62",
   "metadata": {},
   "source": [
    "# Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6453a6-db49-4cca-9c0f-7420cfb5d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 -- Total Reward : -200.\n",
      "Iteration No: 51 -- Total Reward : -200.\n",
      "Iteration No: 101 -- Total Reward : -200.\n",
      "Iteration No: 151 -- Total Reward : -200.\n",
      "Iteration No: 201 -- Total Reward : -200.\n",
      "Iteration No: 251 -- Total Reward : -200.\n",
      "Iteration No: 301 -- Total Reward : -200.\n",
      "Iteration No: 351 -- Total Reward : -200.\n",
      "Iteration No: 401 -- Total Reward : -200.\n",
      "Iteration No: 451 -- Total Reward : -200.\n",
      "Iteration No: 501 -- Total Reward : -200.\n",
      "Iteration No: 551 -- Total Reward : -200.\n",
      "Iteration No: 601 -- Total Reward : -200.\n",
      "Iteration No: 651 -- Total Reward : -200.\n",
      "Iteration No: 701 -- Total Reward : -200.\n",
      "Iteration No: 751 -- Total Reward : -200.\n",
      "Iteration No: 801 -- Total Reward : -200.\n",
      "Iteration No: 851 -- Total Reward : -200.\n",
      "Iteration No: 901 -- Total Reward : -200.\n",
      "Iteration No: 951 -- Total Reward : -200.\n",
      "Iteration No: 1001 -- Total Reward : -200.\n",
      "Iteration No: 1051 -- Total Reward : -200.\n",
      "Iteration No: 1101 -- Total Reward : -200.\n",
      "Iteration No: 1151 -- Total Reward : -200.\n",
      "Iteration No: 1201 -- Total Reward : -200.\n",
      "Iteration No: 1251 -- Total Reward : -200.\n",
      "Iteration No: 1301 -- Total Reward : -200.\n",
      "Iteration No: 1351 -- Total Reward : -200.\n",
      "Iteration No: 1401 -- Total Reward : -200.\n",
      "Iteration No: 1451 -- Total Reward : -200.\n",
      "Iteration No: 1501 -- Total Reward : -200.\n",
      "Iteration No: 1551 -- Total Reward : -200.\n",
      "Iteration No: 1601 -- Total Reward : -200.\n",
      "Iteration No: 1651 -- Total Reward : -200.\n",
      "Iteration No: 1701 -- Total Reward : -200.\n",
      "Iteration No: 1751 -- Total Reward : -200.\n",
      "Iteration No: 1801 -- Total Reward : -200.\n",
      "Iteration No: 1851 -- Total Reward : -200.\n",
      "Iteration No: 1901 -- Total Reward : -200.\n",
      "Iteration No: 1951 -- Total Reward : -200.\n",
      "Iteration No: 2001 -- Total Reward : -200.\n",
      "Iteration No: 2051 -- Total Reward : -200.\n",
      "Iteration No: 2101 -- Total Reward : -200.\n",
      "Iteration No: 2151 -- Total Reward : -200.\n",
      "Iteration No: 2201 -- Total Reward : -200.\n",
      "Iteration No: 2251 -- Total Reward : -200.\n",
      "Iteration No: 2301 -- Total Reward : -200.\n",
      "Iteration No: 2351 -- Total Reward : -200.\n",
      "Iteration No: 2401 -- Total Reward : -200.\n",
      "Iteration No: 2451 -- Total Reward : -200.\n",
      "Iteration No: 2501 -- Total Reward : -200.\n",
      "Iteration No: 2551 -- Total Reward : -200.\n",
      "Iteration No: 2601 -- Total Reward : -200.\n",
      "Iteration No: 2651 -- Total Reward : -200.\n",
      "Iteration No: 2701 -- Total Reward : -200.\n",
      "Iteration No: 2751 -- Total Reward : -200.\n",
      "Iteration No: 2801 -- Total Reward : -200.\n",
      "Iteration No: 2851 -- Total Reward : -200.\n",
      "Iteration No: 2901 -- Total Reward : -200.\n",
      "Iteration No: 2951 -- Total Reward : -200.\n",
      "Iteration No: 3001 -- Total Reward : -200.\n",
      "Iteration No: 3051 -- Total Reward : -200.\n",
      "Iteration No: 3101 -- Total Reward : -200.\n",
      "Iteration No: 3151 -- Total Reward : -200.\n",
      "Iteration No: 3201 -- Total Reward : -200.\n",
      "Iteration No: 3251 -- Total Reward : -200.\n",
      "Iteration No: 3301 -- Total Reward : -200.\n",
      "Iteration No: 3351 -- Total Reward : -200.\n",
      "Iteration No: 3401 -- Total Reward : -200.\n",
      "Iteration No: 3451 -- Total Reward : -200.\n",
      "Iteration No: 3501 -- Total Reward : -200.\n",
      "Iteration No: 3551 -- Total Reward : -200.\n",
      "Iteration No: 3601 -- Total Reward : -200.\n",
      "Iteration No: 3651 -- Total Reward : -200.\n",
      "Iteration No: 3701 -- Total Reward : -200.\n",
      "Iteration No: 3751 -- Total Reward : -200.\n",
      "Iteration No: 3801 -- Total Reward : -200.\n",
      "Iteration No: 3851 -- Total Reward : -200.\n",
      "Iteration No: 3901 -- Total Reward : -200.\n",
      "Iteration No: 3951 -- Total Reward : -200.\n",
      "Iteration No: 4001 -- Total Reward : -200.\n",
      "Iteration No: 4051 -- Total Reward : -200.\n",
      "Iteration No: 4101 -- Total Reward : -200.\n",
      "Iteration No: 4151 -- Total Reward : -200.\n",
      "Iteration No: 4201 -- Total Reward : -200.\n",
      "Iteration No: 4251 -- Total Reward : -200.\n",
      "Iteration No: 4301 -- Total Reward : -200.\n",
      "Iteration No: 4351 -- Total Reward : -200.\n",
      "Iteration No: 4401 -- Total Reward : -200.\n",
      "Iteration No: 4451 -- Total Reward : -200.\n",
      "Iteration No: 4501 -- Total Reward : -200.\n",
      "Iteration No: 4551 -- Total Reward : -200.\n",
      "Iteration No: 4601 -- Total Reward : -200.\n",
      "Iteration No: 4651 -- Total Reward : -200.\n",
      "Iteration No: 4701 -- Total Reward : -200.\n",
      "Iteration No: 4751 -- Total Reward : -200.\n",
      "Iteration No: 4801 -- Total Reward : -200.\n",
      "Iteration No: 4851 -- Total Reward : -200.\n",
      "Iteration No: 4901 -- Total Reward : -200.\n",
      "Iteration No: 4951 -- Total Reward : -200.\n",
      "Mean score :  -126.89\n"
     ]
    }
   ],
   "source": [
    "'''The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn\n",
    "on two continuous variables: position and velocity. For any given state (position and velocity) of the car, the agent is given the\n",
    "possibility of driving left, driving right, or not using the engine at all'''\n",
    "# https://en.wikipedia.org/wiki/Mountain_car_problem\n",
    "# environment: https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "# 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "# 2 observations: 0:position ; 1:volecity\n",
    "# inspired by https://github.com/llSourcell/Q_Learning_Explained\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "# initializations\n",
    "number_states = 40 # number_of_states\n",
    "max_iteration = 5000 # max_iteration\n",
    "initial_learning_rate = 1.0 # initial learning rate\n",
    "min_learning_rate = 0.005   # minimum learning rate\n",
    "max_step = 10000 # max_step\n",
    "\n",
    "# parameters for q learning\n",
    "epsilon = 0.05\n",
    "gamma = 1.0\n",
    "\n",
    "def observation_to_state(environment, observation):\n",
    "    # map an observation to state\n",
    "    environment_low = environment.observation_space.low\n",
    "    environment_high = environment.observation_space.high\n",
    "    environment_dx = (environment_high - environment_low) / number_states\n",
    "\n",
    "    # observation[0]:position ;  observation[1]: volecity\n",
    "    p = int((observation[0] - environment_low[0])/environment_dx[0])\n",
    "    v = int((observation[1] - environment_low[1])/environment_dx[1])\n",
    "    # p:position, v:volecity\n",
    "    return p, v\n",
    "\n",
    "\n",
    "def episode_simulation(environment, policy=None, render=False):\n",
    "    observation= environment.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    for _ in range(max_step):\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            p,v = observation_to_state(environment, observation)\n",
    "            action = policy[p][v]\n",
    "        if render:\n",
    "            environment.render()\n",
    "        # proceed environment for each step\n",
    "        # get observation, reward and done after each step\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        total_reward += gamma ** step_count * reward\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # use gym environment: MountainCar-v0\n",
    "    # https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "    environment_name = 'MountainCar-v0'\n",
    "    environment = gym.make(environment_name)\n",
    "    environment.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # create qTable with zeros\n",
    "    # 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "    q_table = np.zeros((number_states, number_states, 3))\n",
    "\n",
    "    # training for maximum iteration episodes\n",
    "    for i in range(max_iteration):\n",
    "        observation = environment.reset()\n",
    "        total_reward = 0\n",
    "        # eta: learning rate is decreased at each step\n",
    "        eta = max(min_learning_rate, initial_learning_rate * (0.85 ** (i//100)))\n",
    "        # each episode is max_step long\n",
    "        for j in range(max_step):\n",
    "            p, v = observation_to_state(environment, observation)\n",
    "            # select an action\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n)\n",
    "            else:\n",
    "                logits = q_table[p][v]\n",
    "                # calculate the exponential of all elements in the input array.\n",
    "                logits_exp = np.exp(logits)\n",
    "                # calculate the probabilities\n",
    "                probabilities = logits_exp / np.sum(logits_exp)\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "                # get observation, reward and done after each step\n",
    "            observation, reward, done, _ = environment.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            # p:position, v:volecity\n",
    "            p_, v_ = observation_to_state(environment, observation)\n",
    "            # gamma: discount factor\n",
    "            # Bellmann eq: Q(s,a)=reward + gamma* max(Q(s_,a_))  ::: Q_target = reward+gamma*max(Qs_prime)\n",
    "            q_table[p][v][action] = q_table[p][v][action] + eta * (reward + gamma *  np.max(q_table[p_][v_]) - q_table[p][v][action])\n",
    "            if done:\n",
    "                # print('#Iterations for each episode:', j)\n",
    "                break\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration No: %d -- Total Reward : %d.' %(i+1, total_reward))\n",
    "\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [episode_simulation(environment, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Mean score : \", np.mean(solution_policy_scores))\n",
    "    \n",
    "# run with render=True for visualization\n",
    "episode_simulation(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc33d86-ea36-42bd-aa0b-60ffaee4ab44",
   "metadata": {},
   "source": [
    "## Update: 00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e96bfc32-af48-4492-b0a4-ee6459a8b657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-92.0, -92.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def episode_simulation_basic(environment, policy=None, render=False):\n",
    "    observation= environment.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    total_reward_basic = 0\n",
    "    for i in range(max_step):\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            p,v = observation_to_state(environment, observation)\n",
    "            action = policy[p][v]\n",
    "        if render:\n",
    "            environment.render()\n",
    "        # proceed environment for each step\n",
    "        # get observation, reward and done after each step\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        total_reward += (gamma ** step_count) * reward\n",
    "        step_count += 1\n",
    "        total_reward_basic += reward\n",
    "        #print(i, gamma, step_count, reward, total_reward, total_reward_basic)\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward, total_reward_basic\n",
    "\n",
    "# run with render=True for visualization\n",
    "episode_simulation_basic(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "035c8403-656f-4e75-928b-ff77b657e8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 -- Total Reward : -97.\n",
      "Iteration No: 2 -- Total Reward : -134.\n",
      "Iteration No: 3 -- Total Reward : -131.\n",
      "Iteration No: 4 -- Total Reward : -131.\n",
      "Iteration No: 5 -- Total Reward : -101.\n",
      "Iteration No: 6 -- Total Reward : -134.\n",
      "Iteration No: 7 -- Total Reward : -134.\n",
      "Iteration No: 8 -- Total Reward : -134.\n",
      "Iteration No: 9 -- Total Reward : -98.\n",
      "Iteration No: 10 -- Total Reward : -134.\n"
     ]
    }
   ],
   "source": [
    "policy = solution_policy\n",
    "epsilon = 0\n",
    "max_iteration = 10\n",
    "for i in range(max_iteration):\n",
    "    observation = environment.reset()\n",
    "    total_reward = 0\n",
    "    # eta: learning rate is decreased at each step\n",
    "    eta = max(min_learning_rate, initial_learning_rate * (0.85 ** (i//100)))\n",
    "    # each episode is max_step long\n",
    "    for j in range(max_step):\n",
    "        p, v = observation_to_state(environment, observation)\n",
    "        # select an action\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            # get random action\n",
    "            action = np.random.choice(environment.action_space.n)\n",
    "        else:\n",
    "            \"\"\"\n",
    "            logits = q_table[p][v]\n",
    "            # calculate the exponential of all elements in the input array.\n",
    "            logits_exp = np.exp(logits)\n",
    "            # calculate the probabilities\n",
    "            probabilities = logits_exp / np.sum(logits_exp)\n",
    "            # get random action\n",
    "            action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "            \"\"\"\n",
    "            # get observation, reward and done after each step\n",
    "            # p,v = observation_to_state(environment, observation)\n",
    "            action = policy[p][v]\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        # update q table\n",
    "        # p:position, v:volecity\n",
    "        p_, v_ = observation_to_state(environment, observation)\n",
    "        # gamma: discount factor\n",
    "        # Bellmann eq: Q(s,a)=reward + gamma* max(Q(s_,a_))  ::: Q_target = reward+gamma*max(Qs_prime)\n",
    "        # q_table[p][v][action] = q_table[p][v][action] + eta * (reward + gamma *  np.max(q_table[p_][v_]) - q_table[p][v][action])\n",
    "        environment.render()\n",
    "        if done:\n",
    "            # print('#Iterations for each episode:', j)\n",
    "            break\n",
    "    #if i % 50 == 0:\n",
    "    print('Iteration No: %d -- Total Reward : %d.' %(i+1, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b3533-86fe-4074-adcc-a36959f3155d",
   "metadata": {},
   "source": [
    "## Update: 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148dc8a8-b1af-4cd9-9a66-5251971563d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 -- Total Reward : -200.\n",
      "Iteration No: 51 -- Total Reward : -200.\n",
      "Iteration No: 101 -- Total Reward : -200.\n",
      "Iteration No: 151 -- Total Reward : -200.\n",
      "Iteration No: 201 -- Total Reward : -200.\n",
      "Iteration No: 251 -- Total Reward : -200.\n",
      "Iteration No: 301 -- Total Reward : -200.\n",
      "Iteration No: 351 -- Total Reward : -200.\n",
      "Iteration No: 401 -- Total Reward : -200.\n",
      "Iteration No: 451 -- Total Reward : -200.\n",
      "Iteration No: 501 -- Total Reward : -200.\n",
      "Iteration No: 551 -- Total Reward : -200.\n",
      "Iteration No: 601 -- Total Reward : -200.\n",
      "Iteration No: 651 -- Total Reward : -176.\n",
      "Iteration No: 701 -- Total Reward : -200.\n",
      "Iteration No: 751 -- Total Reward : -197.\n",
      "Iteration No: 801 -- Total Reward : -200.\n",
      "Iteration No: 851 -- Total Reward : -200.\n",
      "Iteration No: 901 -- Total Reward : -200.\n",
      "Iteration No: 951 -- Total Reward : -200.\n",
      "Iteration No: 1001 -- Total Reward : -200.\n",
      "Iteration No: 1051 -- Total Reward : -193.\n",
      "Iteration No: 1101 -- Total Reward : -200.\n",
      "Iteration No: 1151 -- Total Reward : -200.\n",
      "Iteration No: 1201 -- Total Reward : -200.\n",
      "Iteration No: 1251 -- Total Reward : -161.\n",
      "Iteration No: 1301 -- Total Reward : -170.\n",
      "Iteration No: 1351 -- Total Reward : -200.\n",
      "Iteration No: 1401 -- Total Reward : -200.\n",
      "Iteration No: 1451 -- Total Reward : -169.\n",
      "Iteration No: 1501 -- Total Reward : -200.\n",
      "Iteration No: 1551 -- Total Reward : -165.\n",
      "Iteration No: 1601 -- Total Reward : -156.\n",
      "Iteration No: 1651 -- Total Reward : -200.\n",
      "Iteration No: 1701 -- Total Reward : -200.\n",
      "Iteration No: 1751 -- Total Reward : -156.\n",
      "Iteration No: 1801 -- Total Reward : -174.\n",
      "Iteration No: 1851 -- Total Reward : -199.\n",
      "Iteration No: 1901 -- Total Reward : -161.\n",
      "Iteration No: 1951 -- Total Reward : -164.\n",
      "Iteration No: 2001 -- Total Reward : -165.\n",
      "Iteration No: 2051 -- Total Reward : -200.\n",
      "Iteration No: 2101 -- Total Reward : -198.\n",
      "Iteration No: 2151 -- Total Reward : -160.\n",
      "Iteration No: 2201 -- Total Reward : -199.\n",
      "Iteration No: 2251 -- Total Reward : -164.\n",
      "Iteration No: 2301 -- Total Reward : -200.\n",
      "Iteration No: 2351 -- Total Reward : -157.\n",
      "Iteration No: 2401 -- Total Reward : -200.\n",
      "Iteration No: 2451 -- Total Reward : -200.\n",
      "Iteration No: 2501 -- Total Reward : -200.\n",
      "Iteration No: 2551 -- Total Reward : -200.\n",
      "Iteration No: 2601 -- Total Reward : -158.\n",
      "Iteration No: 2651 -- Total Reward : -200.\n",
      "Iteration No: 2701 -- Total Reward : -173.\n",
      "Iteration No: 2751 -- Total Reward : -168.\n",
      "Iteration No: 2801 -- Total Reward : -200.\n",
      "Iteration No: 2851 -- Total Reward : -200.\n",
      "Iteration No: 2901 -- Total Reward : -200.\n",
      "Iteration No: 2951 -- Total Reward : -154.\n",
      "Iteration No: 3001 -- Total Reward : -200.\n",
      "Iteration No: 3051 -- Total Reward : -197.\n",
      "Iteration No: 3101 -- Total Reward : -200.\n",
      "Iteration No: 3151 -- Total Reward : -155.\n",
      "Iteration No: 3201 -- Total Reward : -200.\n",
      "Iteration No: 3251 -- Total Reward : -150.\n",
      "Iteration No: 3301 -- Total Reward : -200.\n",
      "Iteration No: 3351 -- Total Reward : -168.\n",
      "Iteration No: 3401 -- Total Reward : -200.\n",
      "Iteration No: 3451 -- Total Reward : -165.\n",
      "Iteration No: 3501 -- Total Reward : -171.\n",
      "Iteration No: 3551 -- Total Reward : -200.\n",
      "Iteration No: 3601 -- Total Reward : -153.\n",
      "Iteration No: 3651 -- Total Reward : -175.\n",
      "Iteration No: 3701 -- Total Reward : -147.\n",
      "Iteration No: 3751 -- Total Reward : -157.\n",
      "Iteration No: 3801 -- Total Reward : -200.\n",
      "Iteration No: 3851 -- Total Reward : -150.\n",
      "Iteration No: 3901 -- Total Reward : -124.\n",
      "Iteration No: 3951 -- Total Reward : -200.\n",
      "Iteration No: 4001 -- Total Reward : -200.\n",
      "Iteration No: 4051 -- Total Reward : -200.\n",
      "Iteration No: 4101 -- Total Reward : -200.\n",
      "Iteration No: 4151 -- Total Reward : -157.\n",
      "Iteration No: 4201 -- Total Reward : -200.\n",
      "Iteration No: 4251 -- Total Reward : -162.\n",
      "Iteration No: 4301 -- Total Reward : -152.\n",
      "Iteration No: 4351 -- Total Reward : -200.\n",
      "Iteration No: 4401 -- Total Reward : -200.\n",
      "Iteration No: 4451 -- Total Reward : -169.\n",
      "Iteration No: 4501 -- Total Reward : -157.\n",
      "Iteration No: 4551 -- Total Reward : -167.\n",
      "Iteration No: 4601 -- Total Reward : -171.\n",
      "Iteration No: 4651 -- Total Reward : -167.\n",
      "Iteration No: 4701 -- Total Reward : -200.\n",
      "Iteration No: 4751 -- Total Reward : -200.\n",
      "Iteration No: 4801 -- Total Reward : -200.\n",
      "Iteration No: 4851 -- Total Reward : -200.\n",
      "Iteration No: 4901 -- Total Reward : -156.\n",
      "Iteration No: 4951 -- Total Reward : -200.\n",
      "Mean score :  -192.99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-200.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn\n",
    "on two continuous variables: position and velocity. For any given state (position and velocity) of the car, the agent is given the\n",
    "possibility of driving left, driving right, or not using the engine at all'''\n",
    "# https://en.wikipedia.org/wiki/Mountain_car_problem\n",
    "# environment: https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "# 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "# 2 observations: 0:position ; 1:volecity\n",
    "# inspired by https://github.com/llSourcell/Q_Learning_Explained\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "# initializations\n",
    "number_states = 40 # number_of_states\n",
    "max_iteration = 5000 # max_iteration\n",
    "initial_learning_rate = 1.0 # initial learning rate\n",
    "min_learning_rate = 0.005   # minimum learning rate\n",
    "max_step = 10000 # max_step\n",
    "\n",
    "# parameters for q learning\n",
    "epsilon = 0.05\n",
    "gamma = 1.0\n",
    "\n",
    "def observation_to_state(environment, observation):\n",
    "    # map an observation to state\n",
    "    environment_low = environment.observation_space.low\n",
    "    environment_high = environment.observation_space.high\n",
    "    environment_dx = (environment_high - environment_low) / number_states\n",
    "\n",
    "    # observation[0]:position ;  observation[1]: volecity\n",
    "    p = int((observation[0] - environment_low[0])/environment_dx[0])\n",
    "    v = int((observation[1] - environment_low[1])/environment_dx[1])\n",
    "    # p:position, v:volecity\n",
    "    return p, v\n",
    "\n",
    "\n",
    "def episode_simulation(environment, policy=None, render=False):\n",
    "    observation= environment.reset()\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    for _ in range(max_step):\n",
    "        if policy is None:\n",
    "            action = environment.action_space.sample()\n",
    "        else:\n",
    "            p,v = observation_to_state(environment, observation)\n",
    "            action = policy[p][v]\n",
    "        if render:\n",
    "            environment.render()\n",
    "        # proceed environment for each step\n",
    "        # get observation, reward and done after each step\n",
    "        observation, reward, done, _ = environment.step(action)\n",
    "        total_reward += gamma ** step_count * reward\n",
    "        step_count += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # use gym environment: MountainCar-v0\n",
    "    # https://github.com/openai/gym/wiki/MountainCar-v0\n",
    "    environment_name = 'MountainCar-v0'\n",
    "    environment = gym.make(environment_name)\n",
    "    environment.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # create qTable with zeros\n",
    "    # 3 actions: 0:push_left, 1:no_push, 2:push_right\n",
    "    q_table = np.zeros((number_states, number_states, 3))\n",
    "\n",
    "    # training for maximum iteration episodes\n",
    "    for i in range(max_iteration):\n",
    "        observation = environment.reset()\n",
    "        total_reward = 0\n",
    "        # eta: learning rate is decreased at each step\n",
    "        eta = max(min_learning_rate, initial_learning_rate * (0.85 ** (i//100)))\n",
    "        # each episode is max_step long\n",
    "        for j in range(max_step):\n",
    "            policy = np.argmax(q_table, axis=2)\n",
    "            p, v = observation_to_state(environment, observation)\n",
    "            # select an action\n",
    "            if np.random.uniform(0, 1) < epsilon:\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n)\n",
    "            else:\n",
    "                \"\"\"\n",
    "                logits = q_table[p][v]\n",
    "                # calculate the exponential of all elements in the input array.\n",
    "                logits_exp = np.exp(logits)\n",
    "                # calculate the probabilities\n",
    "                probabilities = logits_exp / np.sum(logits_exp)\n",
    "                # get random action\n",
    "                action = np.random.choice(environment.action_space.n, p=probabilities)\n",
    "                \"\"\"\n",
    "                # get observation, reward and done after each step\n",
    "                # p,v = observation_to_state(environment, observation)\n",
    "                action = policy[p][v]\n",
    "            observation, reward, done, _ = environment.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            # update q table\n",
    "            # p:position, v:volecity\n",
    "            p_, v_ = observation_to_state(environment, observation)\n",
    "            # gamma: discount factor\n",
    "            # Bellmann eq: Q(s,a)=reward + gamma* max(Q(s_,a_))  ::: Q_target = reward+gamma*max(Qs_prime)\n",
    "            q_table[p][v][action] = q_table[p][v][action] + eta * (reward + gamma *  np.max(q_table[p_][v_]) - q_table[p][v][action])\n",
    "            if done:\n",
    "                # print('#Iterations for each episode:', j)\n",
    "                break\n",
    "        if i % 50 == 0:\n",
    "            print('Iteration No: %d -- Total Reward : %d.' %(i+1, total_reward))\n",
    "\n",
    "    solution_policy = np.argmax(q_table, axis=2)\n",
    "    solution_policy_scores = [episode_simulation(environment, solution_policy, False) for _ in range(100)]\n",
    "    print(\"Mean score : \", np.mean(solution_policy_scores))\n",
    "    \n",
    "# run with render=True for visualization\n",
    "episode_simulation(environment, solution_policy, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ce9f1c-22d5-4b28-92ed-59ffb7a18d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
