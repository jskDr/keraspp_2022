{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 필기체를 생성하는 합성곱 계층 GAN 구현 \n",
    "GAN을 이용해 필기체 숫자를 생성하는 인공신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 공통 패키지 불러오기\n",
    "- 공통 패키지 MNIST 데이터셋을 불러오는 케라스 서브패키지를 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras import models, layers, optimizers\n",
    "\n",
    "# The Conv2D op currently only supports the NHWC tensor format on the CPU\n",
    "import keras.backend as K\n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 사용자 정의 손실 함수 만들기\n",
    "- 케라스가 제공하지 않는 함수를 사용자 정의로 만듬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 합성곱 계층 GAN 모델링\n",
    "- GAN에 포함된 2가지 신경망인 생성망과 판별망을 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=32): # input_dim = args.n_train = 32\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
    "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(28, 28, 1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = np.array([1] * ln + [0] * ln).reshape(-1,1) # Necessary!\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 합성곱 계층 GAN 학습 시키기\n",
    "- 앞서 만든 GAN의 모델링과 학습용 클래스를 사용해서 실제로 GAN을 학습하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[2:]\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[0, :, :]\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format \n",
    "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch is\", epoch)\n",
    "            print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.5 합성곱 계층 GAN 수행 \n",
    "- 각 단계별 구현에 앞서 합성곱 계층 GAN의 수행 방법과 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_fold is GAN_OUT\n",
      "Epoch is 0\n",
      "Number of batches 2\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 10\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2.6 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function image_data_format at 0x7fbcdbd9ce50>\n",
      "Output_fold is GAN_OUT\n",
      "Epoch is 0\n",
      "Number of batches 2\n",
      "Epoch is 10\n",
      "Number of batches 2\n",
      "Epoch is 20\n",
      "Number of batches 2\n",
      "Epoch is 30\n",
      "Number of batches 2\n",
      "Epoch is 40\n",
      "Number of batches 2\n",
      "Epoch is 50\n",
      "Number of batches 2\n",
      "Epoch is 60\n",
      "Number of batches 2\n",
      "Epoch is 70\n",
      "Number of batches 2\n",
      "Epoch is 80\n",
      "Number of batches 2\n",
      "Epoch is 90\n",
      "Number of batches 2\n",
      "Epoch is 100\n",
      "Number of batches 2\n",
      "Epoch is 110\n",
      "Number of batches 2\n",
      "Epoch is 120\n",
      "Number of batches 2\n",
      "Epoch is 130\n",
      "Number of batches 2\n",
      "Epoch is 140\n",
      "Number of batches 2\n",
      "Epoch is 150\n",
      "Number of batches 2\n",
      "Epoch is 160\n",
      "Number of batches 2\n",
      "Epoch is 170\n",
      "Number of batches 2\n",
      "Epoch is 180\n",
      "Number of batches 2\n",
      "Epoch is 190\n",
      "Number of batches 2\n",
      "Epoch is 200\n",
      "Number of batches 2\n",
      "Epoch is 210\n",
      "Number of batches 2\n",
      "Epoch is 220\n",
      "Number of batches 2\n",
      "Epoch is 230\n",
      "Number of batches 2\n",
      "Epoch is 240\n",
      "Number of batches 2\n",
      "Epoch is 250\n",
      "Number of batches 2\n",
      "Epoch is 260\n",
      "Number of batches 2\n",
      "Epoch is 270\n",
      "Number of batches 2\n",
      "Epoch is 280\n",
      "Number of batches 2\n",
      "Epoch is 290\n",
      "Number of batches 2\n",
      "Epoch is 300\n",
      "Number of batches 2\n",
      "Epoch is 310\n",
      "Number of batches 2\n",
      "Epoch is 320\n",
      "Number of batches 2\n",
      "Epoch is 330\n",
      "Number of batches 2\n",
      "Epoch is 340\n",
      "Number of batches 2\n",
      "Epoch is 350\n",
      "Number of batches 2\n",
      "Epoch is 360\n",
      "Number of batches 2\n",
      "Epoch is 370\n",
      "Number of batches 2\n",
      "Epoch is 380\n",
      "Number of batches 2\n",
      "Epoch is 390\n",
      "Number of batches 2\n",
      "Epoch is 400\n",
      "Number of batches 2\n",
      "Epoch is 410\n",
      "Number of batches 2\n",
      "Epoch is 420\n",
      "Number of batches 2\n",
      "Epoch is 430\n",
      "Number of batches 2\n",
      "Epoch is 440\n",
      "Number of batches 2\n",
      "Epoch is 450\n",
      "Number of batches 2\n",
      "Epoch is 460\n",
      "Number of batches 2\n",
      "Epoch is 470\n",
      "Number of batches 2\n",
      "Epoch is 480\n",
      "Number of batches 2\n",
      "Epoch is 490\n",
      "Number of batches 2\n",
      "Epoch is 500\n",
      "Number of batches 2\n",
      "Epoch is 510\n",
      "Number of batches 2\n",
      "Epoch is 520\n",
      "Number of batches 2\n",
      "Epoch is 530\n",
      "Number of batches 2\n",
      "Epoch is 540\n",
      "Number of batches 2\n",
      "Epoch is 550\n",
      "Number of batches 2\n",
      "Epoch is 560\n",
      "Number of batches 2\n",
      "Epoch is 570\n",
      "Number of batches 2\n",
      "Epoch is 580\n",
      "Number of batches 2\n",
      "Epoch is 590\n",
      "Number of batches 2\n",
      "Epoch is 600\n",
      "Number of batches 2\n",
      "Epoch is 610\n",
      "Number of batches 2\n",
      "Epoch is 620\n",
      "Number of batches 2\n",
      "Epoch is 630\n",
      "Number of batches 2\n",
      "Epoch is 640\n",
      "Number of batches 2\n",
      "Epoch is 650\n",
      "Number of batches 2\n",
      "Epoch is 660\n",
      "Number of batches 2\n",
      "Epoch is 670\n",
      "Number of batches 2\n",
      "Epoch is 680\n",
      "Number of batches 2\n",
      "Epoch is 690\n",
      "Number of batches 2\n",
      "Epoch is 700\n",
      "Number of batches 2\n",
      "Epoch is 710\n",
      "Number of batches 2\n",
      "Epoch is 720\n",
      "Number of batches 2\n",
      "Epoch is 730\n",
      "Number of batches 2\n",
      "Epoch is 740\n",
      "Number of batches 2\n",
      "Epoch is 750\n",
      "Number of batches 2\n",
      "Epoch is 760\n",
      "Number of batches 2\n",
      "Epoch is 770\n",
      "Number of batches 2\n",
      "Epoch is 780\n",
      "Number of batches 2\n",
      "Epoch is 790\n",
      "Number of batches 2\n",
      "Epoch is 800\n",
      "Number of batches 2\n",
      "Epoch is 810\n",
      "Number of batches 2\n",
      "Epoch is 820\n",
      "Number of batches 2\n",
      "Epoch is 830\n",
      "Number of batches 2\n",
      "Epoch is 840\n",
      "Number of batches 2\n",
      "Epoch is 850\n",
      "Number of batches 2\n",
      "Epoch is 860\n",
      "Number of batches 2\n",
      "Epoch is 870\n",
      "Number of batches 2\n",
      "Epoch is 880\n",
      "Number of batches 2\n",
      "Epoch is 890\n",
      "Number of batches 2\n",
      "Epoch is 900\n",
      "Number of batches 2\n",
      "Epoch is 910\n",
      "Number of batches 2\n",
      "Epoch is 920\n",
      "Number of batches 2\n",
      "Epoch is 930\n",
      "Number of batches 2\n",
      "Epoch is 940\n",
      "Number of batches 2\n",
      "Epoch is 950\n",
      "Number of batches 2\n",
      "Epoch is 960\n",
      "Number of batches 2\n",
      "Epoch is 970\n",
      "Number of batches 2\n",
      "Epoch is 980\n",
      "Number of batches 2\n",
      "Epoch is 990\n",
      "Number of batches 2\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "# 공통 패키지 불러오기\n",
    "################################\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "\n",
    "from keras import models, layers, optimizers\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "\n",
    "print(K.image_data_format)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def mse_4d(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis=(1,2,3))\n",
    "\n",
    "################################\n",
    "# GAN 모델링\n",
    "################################\n",
    "class GAN(models.Sequential):\n",
    "    def __init__(self, input_dim=32): # input_dim = args.n_train = 32\n",
    "        \"\"\"\n",
    "        self, self.generator, self.discriminator are all models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False\n",
    "        self.add(self.discriminator)\n",
    "        \n",
    "        self.compile_all()\n",
    "\n",
    "    def compile_all(self):\n",
    "        # Compiling stage\n",
    "        d_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        g_optim = optimizers.SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "        self.generator.compile(loss=mse_4d_tf, optimizer=\"SGD\")\n",
    "        self.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "        self.discriminator.trainable = True\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "\n",
    "    def GENERATOR(self):\n",
    "        input_dim = self.input_dim\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(1024, activation='tanh', input_dim=input_dim))\n",
    "        model.add(layers.Dense(7 * 7 * 128, activation='tanh')) # H, W, C = 7, 7, 128\n",
    "        model.add(layers.BatchNormalization())\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU.\n",
    "        model.add(layers.Reshape((7, 7, 128), input_shape=(7 * 7 * 128,)))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh'))\n",
    "        model.add(layers.UpSampling2D(size=(2, 2)))\n",
    "        model.add(layers.Conv2D(1, (5, 5), padding='same', activation='tanh'))\n",
    "        return model\n",
    "\n",
    "    def DISCRIMINATOR(self):\n",
    "        # The Conv2D op currently only supports the NHWC tensor format on the CPU. \n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Conv2D(64, (5, 5), padding='same', activation='tanh',\n",
    "                                input_shape=(28, 28, 1)))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Conv2D(128, (5, 5), activation='tanh'))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1024, activation='tanh'))\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "        return model\n",
    "\n",
    "    def get_z(self, ln):\n",
    "        input_dim = self.input_dim\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "\n",
    "    def train_both(self, x):\n",
    "        ln = x.shape[0]\n",
    "        # First trial for training discriminator\n",
    "        z = self.get_z(ln)\n",
    "        w = self.generator.predict(z, verbose=0)\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = np.array([1] * ln + [0] * ln).reshape(-1,1) # Necessary!\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # Second trial for training generator\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False\n",
    "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
    "        self.discriminator.trainable = True\n",
    "\n",
    "        return d_loss, g_loss\n",
    "\n",
    "################################\n",
    "# GAN 학습하기\n",
    "################################\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width))\n",
    "    shape = generated_images.shape[1:3] # (1,2) for NHWC\n",
    "    image = np.zeros((height * shape[0], width * shape[1]),\n",
    "                     dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index / width)\n",
    "        j = index % width\n",
    "        image[i * shape[0]:(i + 1) * shape[0],\n",
    "        j * shape[1]:(j + 1) * shape[1]] = img[ :, :, 0] # NHWC\n",
    "    return image\n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE):\n",
    "    return X_train[index * BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index):\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + \"_\" + str(index) + \".png\")\n",
    "\n",
    "def load_data(n_train):\n",
    "    (X_train, y_train), (_, _) = mnist.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args):\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    output_fold = args.output_fold\n",
    "    input_dim = args.input_dim\n",
    "    n_train = args.n_train\n",
    "\n",
    "    os.makedirs(output_fold, exist_ok=True)\n",
    "    print('Output_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # The Conv2D op currently only supports the NHWC tensor format on the CPU. The op was given the format: NCHW\n",
    "    # X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1:]) # <-- NCHW format \n",
    "    X_train = X_train.reshape(X_train.shape + (1,)) # <-- NHWC format\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs):\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch is\", epoch)\n",
    "            print(\"Number of batches\", int(X_train.shape[0] / BATCH_SIZE))\n",
    "\n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)):\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss)\n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose=0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    # gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    # gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)\n",
    "\n",
    "\n",
    "################################\n",
    "# GAN 예제 실행하기\n",
    "################################\n",
    "# import argparse\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    # Not implemented in Notebook\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "        help='Batch size for the networks')\n",
    "    parser.add_argument('--epochs', type=int, default=1000,\n",
    "        help='Epochs for the networks')\n",
    "    parser.add_argument('--output_fold', type=str, default='GAN_OUT',\n",
    "        help='Output fold to save the results')\n",
    "    parser.add_argument('--input_dim', type=int, default=10,\n",
    "        help='Input dimension for the generator.')\n",
    "    parser.add_argument('--n_train', type=int, default=32,\n",
    "        help='The number of training data.')\n",
    "    \"\"\"\n",
    "    class ARGS:\n",
    "        def __init__(args):\n",
    "            args.batch_size = 16\n",
    "            args.epochs = 1000\n",
    "            args.output_fold = 'GAN_OUT'\n",
    "            args.input_dim = 10\n",
    "            args.n_train = 32\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "keras-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
